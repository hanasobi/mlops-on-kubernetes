FROM python:3.11-slim

LABEL maintainer="your-org"
LABEL purpose="vllm-lora-training"
LABEL description="Image for LoRA fine-tuning of LLMs with QLoRA, MLflow tracking"

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    unzip \
    jq \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# PyTorch with CUDA support (largest dependency, separate layer for caching)
RUN pip install --no-cache-dir \
    torch \
    --index-url https://download.pytorch.org/whl/cu121

# HuggingFace training stack
RUN pip install --no-cache-dir \
    transformers \
    peft \
    bitsandbytes \
    datasets \
    accelerate

# MLflow + S3 access
RUN pip install --no-cache-dir \
    'mlflow>=2.9,<3.0' \
    boto3

# Install AWS CLI v2 (used for S3 dataset download)
RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" && \
    unzip awscliv2.zip && \
    ./aws/install && \
    rm -rf awscliv2.zip aws

# Copy training scripts
COPY training/vllm/ /scripts/

# Set Python module search path
ENV PYTHONPATH=/scripts

WORKDIR /workspace
